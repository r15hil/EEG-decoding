{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Plotting\n",
    "A notebook to compile results and organise plots and other resources needed for reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set global plotting params here for consistency\n",
    "plt.style.use('eeg_lib/styles.mplstyle')\n",
    "palette = ['#0b528c', '#7170b6', '#b86eb2', '#ec7098', '#ff8572', '#ffaa4f']\n",
    "\n",
    "from cycler import cycler\n",
    "plt.rc('axes', prop_cycle=cycler('color', palette[::2]+palette[-1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_freqs = [7,10,12] # stim freqs used\n",
    "fs = 256 # sampling freq\n",
    "Ns = 256 # number of sample points to consider\n",
    "Nh = 1 # number of harmonics for CCA-based algos\n",
    "\n",
    "index_pos = dict(zip([\"Nc\", \"Ns\", \"Nt\"], range(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Load data from json log files and arrange by frequency. The compiled data is stored in the dictionary `data` whose keys are the stimulus frequencies used and whose values are the data tensors corresponding to trials at those frequencies. Data tensors will be arranged like `Nc x Ns x Nt` (channels x samples x trials). \n",
    "\n",
    "Note that in this project, we only effectively had one channel. Also, all Nt trials would be independent recordings at the same stimulus frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eeg_lib.utils import read_json\n",
    "import json\n",
    "\n",
    "tests = {7: [\"7hz\"], \n",
    "         10: [\"10hz\"], \n",
    "         12: [\"12hz\"]\n",
    "        }\n",
    "\n",
    "# all_data = read_json('/Users/rishil/Desktop/FYP/EEG-decoding/eeg_lib/logs/seven.json')\n",
    "data = {}\n",
    "\n",
    "cleaned = { 7: [],\n",
    "            10: [],\n",
    "            12: []}\n",
    "\n",
    "for f, test_set in tests.items():\n",
    "    print(f,test_set)\n",
    "    for key in all_data[test_set[0]]:\n",
    "        cleaned[f].append(key['eeg_data'])\n",
    "    print(len(cleaned[f]))\n",
    "# print(cleaned)\n",
    "for f, test_set in tests.items():\n",
    "    data[f] = []\n",
    "    if f == 7:\n",
    "        cleaned[f] = cleaned[f][1:6]\n",
    "    for test in cleaned[f]:\n",
    "        values = test\n",
    "        proc_data = np.array(values)\n",
    "        data[f].append(proc_data.reshape((1, Ns, -1))) # exclude first trial\n",
    "        \n",
    "# del all_data    \n",
    "\n",
    "for f, proc_data in data.items():\n",
    "    if len(proc_data) <= 1:\n",
    "        data[f] = proc_data[0]\n",
    "    else:\n",
    "        data[f] = np.concatenate([*proc_data], axis=-1) # merge data from across trials\n",
    "    print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eeg_lib.utils import read_json\n",
    "import json\n",
    "\n",
    "tests = {7: [\"7hz\"], \n",
    "         10: [\"10hz\"], \n",
    "         12: [\"12hz\"]\n",
    "        }\n",
    "\n",
    "all_data = read_json('/Users/rishil/Desktop/FYP/EEG-decoding/eeg_lib/logs/combined.json')\n",
    "# all_data = read_json('/Users/rishil/Desktop/FYP/EEG-decoding/eeg_lib/logs/moreDataCombined.json')\n",
    "data = {}\n",
    "\n",
    "for f, test_set in tests.items():\n",
    "    data[f] = []\n",
    "    \n",
    "    for test in test_set:\n",
    "        values = all_data[test]\n",
    "        proc_data = np.array([values[i] for i in range(len(values))])\n",
    "        data[f].append(proc_data[1:, :Ns].reshape((1, Ns, -1))) # exclude first trial\n",
    "\n",
    "\n",
    "# del all_data    \n",
    "\n",
    "for f, proc_data in data.items():\n",
    "    if len(proc_data) <= 1:\n",
    "        data[f] = proc_data[0]\n",
    "    else:\n",
    "        data[f] = np.concatenate([*proc_data], axis=-1) # merge data from across trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding\n",
    "Run various decoding algos on gathered data and store results for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CCA\n",
    "Vanilla CCA with no historical training data used across evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eeg_lib.cca import CCA\n",
    "\n",
    "cca = CCA(stim_freqs, fs, Nh=Nh)\n",
    "\n",
    "cca_results = {f:[] for f in stim_freqs}\n",
    "cca_agg_results = {f:{} for f in stim_freqs}\n",
    "\n",
    "for f in stim_freqs:\n",
    "    data_f = data[f]\n",
    "    for trial in range(1, data[f].shape[index_pos[\"Nt\"]]):\n",
    "        Xi = data_f[:, :, trial]\n",
    "        result = cca.classify(Xi)\n",
    "        result = {k:np.round(v[0], 6) for k,v in result.items()}\n",
    "        \n",
    "        result['trial'] = f'f{f}_{trial}'\n",
    "        result['y'] = f\n",
    "        cca_results[f].append(result)\n",
    "        \n",
    "        # compute CCA result using data aggregated across trials\n",
    "        agg_result = cca.compute_corr(data_f.mean(axis=index_pos[\"Nt\"]))\n",
    "        agg_result = {k:np.round(v[0], 6) for k,v in agg_result.items()}\n",
    "        agg_result['y'] = f\n",
    "        cca_agg_results[f] = agg_result\n",
    "    \n",
    "cca_df = pd.concat([pd.DataFrame(result_set) for result_set in cca_results.values()]).set_index('trial')\n",
    "cca_df['y_hat'] = cca_df[stim_freqs].apply(lambda row: stim_freqs[np.argmax(row)], axis=1)\n",
    "\n",
    "cca_agg_df = pd.DataFrame(list(cca_agg_results.values()))\n",
    "cca_agg_df['y_hat'] = cca_agg_df[stim_freqs].apply(lambda row: stim_freqs[np.argmax(row)], axis=1)\n",
    "\n",
    "cca_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template-based Algorithms \n",
    "This section explores decoding algos that, along with potentially the artificially-generated harmonic reference, include template data based on historical 'training' data. These include GCCA, MsetCCA, TRCA and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data tensor shape:  (3, 1, 256, 9)\n",
      "[[[[-1.953760e-01 -1.911673e+01 -1.541282e+02 ... -3.542836e+02\n",
      "    -3.218673e+02 -2.964787e+02]\n",
      "   [-3.347642e+02 -2.774351e+02 -3.006207e+02 ... -2.374091e+02\n",
      "    -2.431845e+02 -2.258122e+02]\n",
      "   [-2.368607e+02 -1.844483e+02 -1.350509e+02 ... -6.801802e+01\n",
      "    -5.321324e+01 -2.792591e+01]\n",
      "   ...\n",
      "   [-1.393106e+01 -2.183523e+01 -2.857095e+01 ... -2.142533e+01\n",
      "    -9.155948e+00 -1.641454e+01]\n",
      "   [-1.713898e+01 -1.830900e+01 -1.507773e+01 ...  2.313829e+01\n",
      "     2.275521e+01  1.350923e+01]\n",
      "   [ 2.558068e+01  2.589160e+01  1.912623e+01 ...  1.310882e+01\n",
      "     1.450313e+01  1.621704e+01]]]\n",
      "\n",
      "\n",
      " [[[-5.747676e-03 -2.361238e-01 -6.659781e-01 ... -3.310901e+00\n",
      "     6.892056e-01  3.018911e+00]\n",
      "   [ 4.742909e-01  1.172098e+00  5.218964e+00 ...  9.130932e-01\n",
      "     6.983992e+00  9.056180e+00]\n",
      "   [ 1.157295e+01  7.941022e+00  1.158662e+00 ...  3.351502e+00\n",
      "    -8.289492e-01  2.496494e+00]\n",
      "   ...\n",
      "   [ 1.383410e+01  1.013636e+01  1.843514e+01 ...  6.945094e+00\n",
      "     6.905689e+00 -7.912691e+00]\n",
      "   [-1.352964e+00  4.257255e+00 -1.547612e+01 ... -8.349230e+00\n",
      "    -2.508564e-01  2.969843e+00]\n",
      "   [-8.349941e+00 -3.268453e+00 -1.042124e+00 ...  8.394219e+00\n",
      "    -6.404297e+00 -2.180650e+00]]]\n",
      "\n",
      "\n",
      " [[[-4.007995e-03 -4.965343e-01 -2.926950e+00 ... -5.114518e+00\n",
      "     4.426324e+00  3.029070e+00]\n",
      "   [ 2.045307e+00  3.377211e+00  8.170462e+00 ... -3.458148e+00\n",
      "     1.841615e+00  1.313697e+01]\n",
      "   [ 5.696510e+00  1.034219e+00  8.923153e+00 ... -5.243061e+00\n",
      "    -9.327167e+00 -1.128807e+01]\n",
      "   ...\n",
      "   [-8.987631e+00 -2.290411e+00 -2.746024e+00 ... -9.684799e+00\n",
      "     3.766616e+00  3.269344e+00]\n",
      "   [ 1.114116e+01  9.430420e+00 -2.801050e+00 ...  9.666347e+00\n",
      "     3.429851e+00  2.087252e+00]\n",
      "   [ 6.879894e+00 -1.428311e+00 -7.921753e-01 ...  5.088890e+00\n",
      "     7.346438e+00  3.265897e+00]]]]\n"
     ]
    }
   ],
   "source": [
    "min_trial_len = np.min([test_set.shape[-1] for test_set in data.values()])\n",
    "\n",
    "# Nf x Nc x Ns x Nt\n",
    "data_tensor = np.array([test_set[:, :, :min_trial_len] for test_set in data.values()])\n",
    "\n",
    "print(\"Data tensor shape: \", data_tensor.shape)\n",
    "print(data_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeavePOut\n",
    "\n",
    "N_train = 4\n",
    "lpo = LeavePOut(p=N_train)\n",
    "\n",
    "n_trials = data_tensor.shape[-1]\n",
    "template_idxs = list(lpo.split(range(n_trials)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data tensor shape:  (3, 1, 256, 10)\n",
      "(3, 1, 256, 4)\n",
      "0 0\n",
      "CORRECT: 7\n",
      "0 1\n",
      "CORRECT: 7\n",
      "0 2\n",
      "CORRECT: 7\n",
      "0 3\n",
      "CORRECT: 7\n",
      "0 4\n",
      "CORRECT: 7\n",
      "0 5\n",
      "CORRECT: 7\n",
      "1 0\n",
      "WRONG: 12\n",
      "1 1\n",
      "CORRECT: 10\n",
      "1 2\n",
      "CORRECT: 10\n",
      "1 3\n",
      "CORRECT: 10\n",
      "1 4\n",
      "CORRECT: 10\n",
      "1 5\n",
      "CORRECT: 10\n",
      "2 0\n",
      "CORRECT: 12\n",
      "2 1\n",
      "CORRECT: 12\n",
      "2 2\n",
      "CORRECT: 12\n",
      "2 3\n",
      "CORRECT: 12\n",
      "2 4\n",
      "CORRECT: 12\n",
      "2 5\n",
      "CORRECT: 12\n",
      "GCCA {'correct': 7, 'incorrect': 11}\n",
      "Mset {'correct': 17, 'incorrect': 1}\n"
     ]
    }
   ],
   "source": [
    "from eeg_lib.cca import GCCA_SSVEP\n",
    "from eeg_lib.cca import MsetCCA_SSVEP\n",
    "                  \n",
    "stim_freqs = [7,10,12]\n",
    "fs= 256\n",
    "Nh = 1\n",
    "\n",
    "min_trial_len = np.min([test_set.shape[-1] for test_set in data.values()])\n",
    "\n",
    "# Nf x Nc x Ns x Nt\n",
    "data_tensor = np.array([test_set[:, :, :min_trial_len] for test_set in data.values()])\n",
    "\n",
    "print(\"Data tensor shape: \", data_tensor.shape)\n",
    "\n",
    "gcca = GCCA_SSVEP(stim_freqs, fs, Nh=Nh)\n",
    "mset_cca = MsetCCA_SSVEP(stim_freqs)\n",
    "\n",
    "chi_train = data_tensor[:, :, :, [6,7,8,9]]\n",
    "print(chi_train.shape)\n",
    "                        \n",
    "gcca.fit(chi_train)\n",
    "mset_cca.fit(chi_train)\n",
    "\n",
    "X_test = data_tensor[0, :, :, 6]\n",
    "\n",
    "results_gcca = {'correct':0,\n",
    "                'incorrect':0}\n",
    "results_mset = {'correct':0,\n",
    "                'incorrect':0}\n",
    "\n",
    "for i in range(data_tensor.shape[0]):\n",
    "    for j in range(6):\n",
    "        print(i,j)\n",
    "        X_test = data_tensor[i, :, :, j]\n",
    "#         print(\"=============================================\")\n",
    "#         print(X_test)\n",
    "#         print(\"=============================================\")\n",
    "        gcca_res = gcca.classify(X_test)\n",
    "        mset_res = mset_cca.classify(X_test)\n",
    "\n",
    "        highest_gcca = 0\n",
    "        highest_gcca_freq = -1\n",
    "        for freq, acc in gcca_res.items():\n",
    "            if abs(acc) > highest_gcca:\n",
    "                highest_gcca_freq = freq\n",
    "                highest_gcca = abs(acc)\n",
    "\n",
    "        if stim_freqs[i] == highest_gcca_freq:\n",
    "            results_gcca['correct']+=1\n",
    "        else:\n",
    "            results_gcca['incorrect']+=1\n",
    "\n",
    "        highest_mset = 0\n",
    "        highest_mset_freq = -1\n",
    "        for freq, acc in mset_res.items():\n",
    "            if abs(acc) > highest_mset:\n",
    "                highest_mset_freq = freq\n",
    "                highest_mset = abs(acc)\n",
    "        if stim_freqs[i] == highest_mset_freq:\n",
    "            results_mset['correct']+=1\n",
    "            print(\"CORRECT:\", highest_mset_freq)\n",
    "        else:\n",
    "            print(\"WRONG:\", highest_mset_freq)\n",
    "            results_mset['incorrect']+=1\n",
    "\n",
    "print(\"GCCA\", results_gcca)\n",
    "print(\"Mset\", results_mset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCCA\n",
    "Generalised CCA aims to simultaneously maximise correlation between three sets of data: historical observations, measured signals in a new sample and the pre-constructed sinusoidal reference. As interpreted by the authors (Wong et al), the optimal spatial filters obtained through GCCA perform SSVEP signal denoising.\n",
    "\n",
    "#### MsetCCA\n",
    "MsetCCA is one extension of standard CCA that takes into account historical data instead of performing inference purely on new observations. Zhang et al propose that this is one of the reasons that standard CCA performs poorly on short time windows; it effectively over fits to localised dynamics. Furthermore, the authors suggest that exclusively using the pre-constructed sinusoidal reference set is not optimal since this artificial reference does not exclude other features from real EEG data. To circumvent this, MsetCCA seeks to optimise the reference signals used in the CCA algorithm by learning multiple linear transforms to maximise overall correlation between canonical variables over many sets of EEG data at each candidate frequency fk ∈ F. This optimisation effectively finds optimal joint spatial filters w1, . . . , wNt (over Nt trials) using only historical observations (‘training’ data). The authors claim that MsetCCA outperforms similar techniques, especially in cases with few channels and short time windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    exps = np.exp(X)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def cross_entropy(X, y):\n",
    "    \"\"\"\n",
    "    source: https://deepnotes.io/softmax-crossentropy\n",
    "    \n",
    "    X is the output from fully connected layer (num_examples x num_classes)\n",
    "    y is labels (num_examples x 1)\n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    p = softmax(X)\n",
    "    log_likelihood = -np.log(p[range(m),y])\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCCA:  {7: 1.0, 10: 1.0, 12: 1.0}\n",
      "MsetCCA:  {7: 1.0, 10: 1.0, 12: 1.0}\n"
     ]
    }
   ],
   "source": [
    "from eeg_lib.cca import GCCA_SSVEP\n",
    "from eeg_lib.cca import MsetCCA_SSVEP\n",
    "\n",
    "def compute_gcca_msetcca_results(gcca, mset_cca, data_tensor, stim_freqs, template_idxs, ce_loss=True):\n",
    "    \n",
    "    gcca_results = {f:[] for f in stim_freqs}\n",
    "    mset_cca_results = {f:[] for f in stim_freqs}\n",
    "    count = 0\n",
    "    for f_idx, f in enumerate(stim_freqs):\n",
    "        for split_idx, (test_idxs, train_idxs) in enumerate(template_idxs):\n",
    "            chi_train = data_tensor[:, :, :, train_idxs]\n",
    "\n",
    "            # train models on current train-test split\n",
    "            gcca.fit(chi_train)\n",
    "            mset_cca.fit(chi_train)\n",
    "\n",
    "            # extract test matrices from all test indices and compute result\n",
    "            for test_idx in test_idxs:\n",
    "                if test_idx in train_idxs:\n",
    "                    raise ValueError(\"Found intersection between train and test indices\")\n",
    "                    \n",
    "                # note: we must match the number of samples Ns in chi_train\n",
    "                X_test = data_tensor[f_idx, :, :, test_idx]\n",
    "                \n",
    "                _idx = f'f{f}_split{split_idx+1}_test{test_idx+1}'\n",
    "                test_meta = {'idx': _idx, 'y': f, 'test': test_idx, 'split': split_idx}\n",
    "                \n",
    "                # GCCA\n",
    "                result = {k: abs(np.round(v,4)) for k,v in gcca.classify(X_test).items()}\n",
    "                gcca_results[f].append({**result, **test_meta})\n",
    "\n",
    "                # MsetCCA\n",
    "                result = {k: abs(np.round(v,4)) for k,v in mset_cca.classify(X_test).items()}\n",
    "                mset_cca_results[f].append({**result, **test_meta})\n",
    "                \n",
    "                break\n",
    "                \n",
    "    def _prep_results_df(results):\n",
    "        df = pd.concat([pd.DataFrame(result_set) for result_set in results.values()])\n",
    "        df['y_hat'] = df[stim_freqs].apply(lambda row: stim_freqs[np.argmax(row)], axis=1)\n",
    "        df = df.set_index(['y', 'split'])\n",
    "        \n",
    "        if ce_loss:\n",
    "            # compute cross entropy loss\n",
    "            for f_idx, f in enumerate(stim_freqs):\n",
    "                result = df.loc[f, stim_freqs].apply(lambda row: cross_entropy(row.values.reshape(1, -1), np.array([f_idx])), axis=1)\n",
    "                df.loc[(f, ), 'ce_loss'] = result.values\n",
    "                \n",
    "        df['correct'] = df.index.get_level_values(level=0) == df.y_hat\n",
    "\n",
    "        return df\n",
    "\n",
    "    gcca_df = _prep_results_df(gcca_results)\n",
    "    mset_df = _prep_results_df(mset_cca_results)\n",
    "    \n",
    "    return gcca_df, mset_df\n",
    "\n",
    "def decoding_acc(result_df):\n",
    "    acc = result_df['correct'].groupby(['y', 'split']).apply(lambda x: np.sum(x)/len(x))\n",
    "    acc_grouped = acc.groupby('y')\n",
    "    acc_av = acc_grouped.mean().to_dict()\n",
    "    acc_std = acc_grouped.std().to_dict()\n",
    "    return {'raw': acc, 'mean': acc_av, 'std': acc_std}\n",
    "\n",
    "gcca = GCCA_SSVEP(stim_freqs, fs, Nh=Nh)\n",
    "mset_cca = MsetCCA_SSVEP(stim_freqs)\n",
    "\n",
    "gcca_df, mset_cca_df = compute_gcca_msetcca_results(gcca, mset_cca, data_tensor, stim_freqs, template_idxs)\n",
    "gcca_acc = decoding_acc(gcca_df)\n",
    "mset_acc = decoding_acc(mset_cca_df)\n",
    "\n",
    "print(\"GCCA: \", gcca_acc['mean'])\n",
    "print(\"MsetCCA: \", mset_acc['mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests to explore\n",
    "\n",
    "Some ideas for interesting tests/factors to investigate. \n",
    "\n",
    "#### Preliminary and verification tests\n",
    "- alpha band tests from Adrian data: periodograms of eyes open vs closed\n",
    "- verification tests using square wave input and filtering both on-device and offline. \n",
    "- verification tests to demonstrate equivalence of downsampling and to show the effect of aliasing without filtering\n",
    "- (possibly included in above two) show periodograms of original unfiltered, filtered and filtered + downsampling\n",
    "\n",
    "#### Main results\n",
    "\n",
    "Test the effect of the following on decoding accuracy:\n",
    "1. number of training trials\n",
    "2. number of samples in each window (Ns)\n",
    "3. number of stimulus frequencies\n",
    "\n",
    "other miscellaneous tests:\n",
    "- [] generalisation performance on different set of data: both with pretraining from diff sets and without\n",
    "- [] average accuracy per stimulus frequency\n",
    "- [] deviation between accuracy of diff frequencies for each trial/test for different algos ** NB\n",
    "- [] average accuracy per stimulus square configuration (wide, narrow etc) * optional\n",
    "- [] some meausre of inter-trial consistency \n",
    "- [] some measure of similarity between estimated outputs: e.g. log loss that penalises similar outputs\n",
    "- [] measure of ITR taking into account Ns per window and overlap between window\n",
    "- [] comparison of periodogram that can't distinguish SSVEP vs CCA that can"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Acc vs number of training trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = 256\n",
    "\n",
    "# Nf x Nc x Ns x Nt\n",
    "data_tensor = np.array([test_set[:,:Ns,:min_trial_len] for test_set in data.values()])\n",
    "\n",
    "print(\"Data tensor shape: \", data_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = data_tensor.shape[-1]\n",
    "gcca_ntr_acc = []\n",
    "mset_ntr_acc = []\n",
    "\n",
    "for n_train in range(1, n_trials):\n",
    "    lpo = LeavePOut(p=n_train)\n",
    "    template_idxs_tmp = list(lpo.split(range(n_trials)))\n",
    "\n",
    "    data_tensor_tmp = data_tensor[:, :, :Ns, :]\n",
    "    gcca_df, mset_cca_df = compute_gcca_msetcca_results(gcca, mset_cca, data_tensor_tmp, stim_freqs, template_idxs_tmp)\n",
    "\n",
    "    _gcca_acc = decoding_acc(gcca_df)\n",
    "    _mset_acc = decoding_acc(mset_cca_df)\n",
    "    \n",
    "    # store these values for easy plotting\n",
    "    gcca_ntr_acc.append({**_gcca_acc['mean'], **{\"n_train\": n_train}})\n",
    "    mset_ntr_acc.append({**_mset_acc['mean'], **{\"n_train\": n_train}})    \n",
    "    \n",
    "acc_ntr_gcca = pd.DataFrame(gcca_ntr_acc).set_index(\"n_train\")\n",
    "acc_ntr_mset = pd.DataFrame(mset_ntr_acc).set_index(\"n_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eeg_lib.plotting import grouped_bar\n",
    "\n",
    "legend = [f'{f}Hz' for f in stim_freqs]\n",
    "\n",
    "fig0, ax0 = plt.subplots(1, figsize=(8,5))\n",
    "_, ax0 = grouped_bar(acc_ntr_gcca.index, acc_ntr_gcca.values*100, xlabel='Training trials', ylabel='Accuracy (%)', ax=ax0)\n",
    "\n",
    "fig1, ax1 = plt.subplots(1, figsize=(8,5))\n",
    "_, ax1 = grouped_bar(acc_ntr_mset.index, acc_ntr_mset.values*100, xlabel='Training trials', ylabel='Accuracy (%)', ax=ax1, colors=palette[1::2])\n",
    "\n",
    "for ax in [ax0, ax1]:\n",
    "    ax.set_yticks(np.arange(20, 110, step=10))\n",
    "    ax.set_ylim(20, 110)\n",
    "    ax.legend(legend, ncol=3, loc='upper left')\n",
    "    \n",
    "ax0.plot(ax0.get_xticks(), acc_ntr_gcca.mean(axis=1)*100, marker='x', c='k', ls='-.', alpha=0.7)\n",
    "ax1.plot(ax1.get_xticks(), acc_ntr_mset.mean(axis=1)*100, marker='x', c='k', ls='-.', alpha=0.7)\n",
    "\n",
    "# save figs\n",
    "fig0.savefig(f'plots/acc_Nt_gcca_Ns{Ns}.pdf', format='pdf')\n",
    "fig1.savefig(f'plots/acc_Nt_mcca_Ns{Ns}.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Acc vs number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 4\n",
    "\n",
    "lpo = LeavePOut(p=N_train)\n",
    "\n",
    "n_trials = data_tensor.shape[-1]\n",
    "template_idxs = list(lpo.split(range(n_trials)))\n",
    "\n",
    "assert data_tensor.shape[-2] == 256, \"Expected data tensor to have full Ns=256 samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_range = [32, 48, 64, 128, 256]\n",
    "gcca_acc = []\n",
    "mset_acc = []\n",
    "\n",
    "for ns in ns_range:\n",
    "    data_tensor_tmp = data_tensor[:, :, :ns, :]\n",
    "    gcca_df, mset_cca_df = compute_gcca_msetcca_results(gcca, mset_cca, data_tensor_tmp, stim_freqs, template_idxs, ce_loss=False)\n",
    "    _gcca_acc = decoding_acc(gcca_df)\n",
    "    _mset_acc = decoding_acc(mset_cca_df)\n",
    "    \n",
    "    # store these values for easy plotting\n",
    "    gcca_acc.append({**_gcca_acc['mean'], **{\"Ns\": ns}})\n",
    "    mset_acc.append({**_mset_acc['mean'], **{\"Ns\": ns}})\n",
    "    \n",
    "acc_ns_gcca = pd.DataFrame(gcca_acc).set_index(\"Ns\")\n",
    "acc_ns_mset = pd.DataFrame(mset_acc).set_index(\"Ns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eeg_lib.plotting import grouped_bar\n",
    "\n",
    "legend = [f'{f}Hz' for f in stim_freqs]\n",
    "t_ax = acc_ns_gcca.index/fs\n",
    "\n",
    "fig0, ax0 = plt.subplots(1, figsize=(8,5))\n",
    "_, ax0 = grouped_bar(t_ax, acc_ns_gcca.values*100, xlabel='window length (s)', ylabel='accuracy (%)', ax=ax0)\n",
    "\n",
    "fig1, ax1 = plt.subplots(1, figsize=(8,5))\n",
    "_, ax1 = grouped_bar(t_ax, acc_ns_mset.values*100, xlabel='window length (s)', ylabel='accuracy (%)', ax=ax1)\n",
    "\n",
    "for ax in [ax0, ax1]:\n",
    "    ax.set_ylim(20, 110)\n",
    "    ax.set_yticks(np.arange(20, 110, step=10))\n",
    "    ax.legend(legend, ncol=3)\n",
    "    \n",
    "ax0.plot(ax0.get_xticks(), acc_ns_gcca.mean(axis=1)*100, marker='x', c='k', ls='-.', alpha=0.7)\n",
    "ax1.plot(ax1.get_xticks(), acc_ns_mset.mean(axis=1)*100, marker='x', c='k', ls='-.', alpha=0.7)\n",
    "# ax1.legend(loc='upper left')\n",
    "\n",
    "# fig0.savefig(f'plots/acc_Ns_gcca_Nt{N_train}.pdf', format='pdf')\n",
    "# fig1.savefig(f'plots/acc_Ns_mcca_Nt{N_train}.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha Band Tests\n",
    "Tests using the 'frankenstein' headset: opening vs closing eyes and looking at energy in the alpha band. No SSVEP, only eyes closed vs eyes open. The section below explores whether the difference in energy in the alpha band (9-10Hz) can be distinguished between these two events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "#load in data\n",
    "from eeg_lib.utils import read_json\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "open_eyes = read_json('/Users/rishil/Desktop/FYP/EEG-decoding/eeg_lib/logs/alpha_tests/alpha_tests_open.json')\n",
    "closed_eyes = read_json('/Users/rishil/Desktop/FYP/EEG-decoding/eeg_lib/logs/alpha_tests/alpha_tests_closed.json')\n",
    "\n",
    "data = {\n",
    "        'open_eyes': [],\n",
    "        'closed_eyes': []\n",
    "       }\n",
    "\n",
    "for i in open_eyes['open_eyes']:\n",
    "    data['open_eyes'].append(np.array(i['eeg_data']))\n",
    "for i in closed_eyes['open_eyes']: # using key open_eyes because recorded with that session id\n",
    "    data['closed_eyes'].append(np.array(i['eeg_data']))\n",
    "\n",
    "print(len(data['open_eyes']))\n",
    "print(len(data['closed_eyes']))\n",
    "\n",
    "data['open_eyes'] = data['open_eyes'][1:11]\n",
    "data['closed_eyes'] = data['closed_eyes'][1:11]\n",
    "\n",
    "start = 0\n",
    "N = 2560\n",
    "Xo = np.squeeze(np.array([d[start:N] for d in data['open_eyes']]))\n",
    "Xc = np.squeeze(np.array([d[start:N] for d in data['closed_eyes']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.append(np.arange(2), np.arange(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eeg_lib.freq_analysis import plot_periodogram\n",
    "\n",
    "\n",
    "Ndft = 1024\n",
    "\n",
    "fig0, axes = plt.subplots(1,2, figsize=(16,4))\n",
    "for i in range(min(Xo.shape[0], 4)):\n",
    "    plot_periodogram(Xo[i, :], 250, N=Ndft, axes=axes)\n",
    "    for ax in axes:\n",
    "        xticks = np.append(np.arange(0, 24, step=4), np.arange(30, 130, step=10))\n",
    "        ax.set_xticks(xticks)\n",
    "        ax.set_xlim(0, 60)\n",
    "        ax.grid(1)\n",
    "\n",
    "fig1, axes = plt.subplots(1,2, figsize=(16,4))\n",
    "for i in range(min(Xc.shape[0], 4)):\n",
    "    plot_periodogram(Xc[i, :], 250, N=Ndft, axes=axes)\n",
    "    for ax in axes:\n",
    "        xticks = np.append(np.arange(0, 24, step=4), np.arange(30, 130, step=10))\n",
    "        ax.set_xticks(xticks)\n",
    "        ax.set_xlim(0, 60)\n",
    "        ax.grid(1)\n",
    "\n",
    "plt.tight_layout(pad=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSP and Filtering\n",
    "Verification tests and demonstration plots of the digital signal processing system including sampling, filtering and downsampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "\n",
    "# data collected from device and copied over\n",
    "x = np.array([2958, 2958, 2957, 2957, 2956, 2955, 2957, 2954, 2955, 2952, 2955, 2955, 2955, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2950, 2951, 2958, 2958, 2955, 2958, 2959, 2956, 2957, 2954, 2953, 2955, 2955, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2959, 2949, 2958, 2957, 2955, 2958, 2958, 2958, 2957, 2960, 2956, 2958, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2955, 2955, 2949, 2959, 2957, 2957, 2959, 0, 0, 0, 0, 0, 0, 2958, 2956, 2957, 2958, 2956, 2956, 2957, 2957, 2957, 2954, 2958, 2957, 2954, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2957, 2955, 2958, 2954, 2955, 2955, 2955, 2956, 2955, 2956, 2956, 2957, 2958, 2955, 2957, 2954, 2955, 2956, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2957, 2957, 2953, 2956, 2955, 2956, 2955, 2951, 2956, 2955, 2955, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2957, 2956, 2957, 2956, 2954, 2953, 2955, 2954, 2954, 2955, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2958, 2956, 2955, 2948, 2947, 2949, 2949, 2950, 2950, 2949, 2951, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2950, 2960, 2957, 2955, 2957, 2955, 2956, 2956, 2955, 2949, 2958, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2959, 2956, 2957, 2957, 2955, 2956, 2957, 2953, 2957, 2955, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2955, 2956, 2954, 2956, 2955, 2955, 2959, 2957, 2959, 2955, 2957, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2947, 2957, 2957, 2957, 2957, 2955, 2957, 2955, 2957, 2959, 2954, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2959, 2947, 2949, 2957, 2955, 2957, 2955, 2957, 2955, 2954, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2956, 2956, 2956, 2955, 2958, 2955, 2947, 2955, 2955, 2954, 2955, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2954, 2955, 2954, 2954, 2954, 2957, 2957, 2957, 2955, 2954, 2953, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2955, 2954, 2954, 2956, 2956, 2955, 2955, 2955, 2955, 2955, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2955, 2954, 2953, 2959, 2955, 2953, 2953, 2954, 2954, 2955, 2955, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2946, 2949, 2957, 2955, 2957, 2955, 2957, 2954, 2954, 2954, 2954, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2958, 2955, 2958, 2956, 2947, 2947, 2959, 2957, 2954, 2955, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2956, 2955, 2955, 2955, 2954, 2958, 2955, 2955, 2954, 2954, 2958, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2958, 2957, 2957, 2954, 2955, 2953, 2955, 2954, 2955, 2955, 2954, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2957, 2954, 2956, 2955, 2958, 2953, 2954, 2958, 2957, 2954, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2960, 2959, 2957, 2956, 2955])\n",
    "x_filt = np.array([0.749196, 3.908255, 13.0332, 33.75168, 74.19135, 143.8398, 251.7594, 403.9336, 600.2468, 832.0441, 1081.352, 1322.543, 1526.632, 1665.978, 1720.654, 1681.619, 1552.216, 1344.726, 1075.958, 761.6009, 412.4424, 34.32355, -367.9954, -784.6752, -1195.291, -1567.171, -1858.14, -2026.219, -2040.636, -1892.818, -1600.272, -1203.909, -757.1008, -310.811, 99.62306, 461.0332, 779.7377, 1070.83, 1344.146, 1591.823, 1785.632, 1883.173, 1843.527, 1643.573, 1290.803, 824.8954, 307.5806, -195.4421, -633.4667, -983.0684, -1248.915, -1453.266, -1617.797, -1748.471, -1827.711, -1820.265, -1687.84, -1408.848, -992.4298, -480.982, 60.30604, 562.8889, 975.2727, 1275.97, 1472.389, 1589.003, 1647.875, 1653.422, 1586.326, 1412.569, 1101.961, 648.6098, 83.82756, -525.262, -1092.073, -1536.016, -1807.126, -1898.597, -1844.595, -1701.702, -1524.497, -1343.015, -1154.069, -927.7673, -628.1702, -236.8592, 228.018, 708.0454, 1115.551, 1358.223, 1367.998, 1122.733, 656.8385, 55.53807, -562.1059, -1068.897, -1355.27, -1350.591, -1036.85, -453.1934, 309.2564, 1122.147, 1843.193, 2343.682, 2537.647, 2402.415, 1986.118, 1394.877, 763.7885, 217.1265, -168.9627, -387.8038, -498.8448, -599.5381, -782.0648, -1091.315, -1499.706, -1912.361, -2199.209, -2244.886, -1994.219, -1477.429, -802.1463, -115.464, 448.9626, 817.4392, 997.4481, 1065.081, 1124.483, 1257.106, 1483.085, 1751.796, 1965.967, 2028.798, 1891.546, 1581.914, 1196.507, 861.6956, 676.5399, 665.5295, 761.1139, 826.5695, 711.8249, 319.9014, -342.53, -1155.07, -1917.295, -2416.985, -2504.126, -2144.565, -1430.574, -545.6141, 301.8329, 948.8148, 1325.603, 1462.156, 1456.248, 1416.701, 1409.964, 1430.325, 1408.001, 1247.769, 880.3521, 303.1651, -407.8372, -1120.395, -1691.015, -2015.695, -2062.177, -1873.689, -1541.763, -1164.305, -806.1368, -479.7798, -153.673, 217.907, 657.1622, 1137.942, 1583.034, 1887.908, 1958.225, 1747.653, 1277.931, 633.6777, -65.30362, -702.415, -1197.748, -1522.652, -1693.914, -1750.008, -1724.48, -1626.681, -1440.83, -1140.695, -712.3898, -173.5533, 421.0309, 990.8127, 1451.84, 1742.725, 1840.331, 1761.534, 1548.812, 1249.857, 899.108, 510.2408, 82.31149, -384.698, -872.298, -1335.742, -1708.274, -1919.121, -1915.536, -1682.382, -1249.144, -682.2899, -66.37628, 518.4391, 1014.033, 1389.916, 1639.012, 1765.0, 1771.21, 1654.359, 1408.556, 1034.956, 552.0494, 0.5509186, -560.771, -1067.618, -1464.783, -1719.142, -1822.964, -1789.297, -1639.8, -1393.969, -1063.234, -653.3639, -173.61, 352.7622, 881.478, 1350.846, 1693.096, 1852.253, 1799.121, 1540.224, 1115.384, 585.957, 18.78627, -527.9962, -1012.701, -1408.62, -1698.592, -1867.34, -1899.107, -1779.896, -1505.941, -1091.951, -574.9527, -10.71037, 537.0494, 1011.044, 1371.511, 1602.0, 1705.169, 1693.714, 1578.701, 1363.871, 1046.966, 628.1962, 121.6132, -436.864, -988.3981, -1461.434, -1787.318, -1918.271, -1838.37, -1566.332, -1147.605, -640.2653, -100.717, 425.5685, 905.9919, 1315.357, 1629.974, 1823.184, 1868.277, 1745.666, 1453.717, 1015.783, 479.9271, -89.72442, -627.472, -1081.133, -1420.574, -1638.976, -1744.238, -1747.645, -1653.041, -1454.619, -1142.69, -715.5066, -191.4045, 385.2965, 947.2011, 1417.992, 1730.166, 1842.37, 1747.342, 1470.677, 1059.806, 569.0621, 47.20063, -468.3088, -949.5639, -1370.684, -1702.33, -1909.901, -1960.115, -1831.156, -1524.433, -1070.143, -523.8008, 45.79834, 573.1322, 1010.862, 1336.077, 1548.229, 1657.721, 1673.528, 1593.536, 1405.125, 1094.062, 658.3486, 120.3818, -469.0839, -1036.827, -1503.954, -1804.745, -1903.091, -1797.983, -1519.424, -1115.446, -636.9947, -127.0803, 382.2641, 865.8003, 1296.396, 1640.212, 1857.026, 1909.417, 1774.755, 1457.275, 992.245, 440.1387, -127.6881, -646.6767, -1073.474, -1390.289, -1600.469, -1715.31, -1741.317, -1671.724, -1489.221, -1176.995, -733.7028, -185.2801, 412.7588, 983.3976, 1446.616, 1738.687, 1827.641, 1716.814, 1438.771, 1041.185, 571.9446, 69.75678, -436.8962, -923.9557, -1362.59, -1715.2, -1937.501, -1989.561, -1848.939, -1522.626, -1049.942, -494.8359, 69.99933, 581.3898, 999.4244, 1310.228, 1519.756, 1639.46, 1673.521, 1611.461, 1432.585, 1118.534, 668.9295, 112.6879, -491.0679, -1062.738, -1521.894, -1806.732, -1888.617, -1774.111, -1497.437, -1105.637, -644.0118, -147.9083, 356.7983, 846.7686, 1291.734, 1651.211, 1877.911, 1930.134, 1785.639, 1453.423, 975.6357, 418.5621, -144.3174, -650.8964, -1063.633, -1371.184, -1581.022, -1704.406, -1743.746, -1686.06, -1508.594, -1192.323, -737.895, -176.4526, 430.4875, 1001.861, 1457.363, 1736.812, 1813.947, 1697.354, 1422.029, 1034.219, 577.2637, 84.31768, -420.1548, -912.8057, -1361.973, -1725.065, -1952.972, -2003.302, -1854.578, -1517.661, -1036.804, -479.6651, 80.25444, 582.1897, 990.6888, 1296.302, 1507.35, 1634.53, 1678.551, 1624.372, 1447.745, 1129.373, 670.9375, 105.4227, -503.8624, -1074.914, -1527.742, -1803.556, -1877.882, -1760.654, -1487.153, -1102.729, -649.0684, -157.6962, 347.7664, 843.6277, 1296.841, 1663.01, 1891.617, 1939.894, 1787.196, 1446.059, 962.5132, 405.3255, -152.0406, -649.9646, -1054.725, -1358.423, -1570.085, -1699.991, -1747.533, -1696.063, -1520.16, -1200.279, -738.8603, -170.2697, 440.816, 1011.653, 1462.445, 1735.41, 1807.41, 1689.435, 1417.07, 1035.104, 584.0834, 94.34014, -411.2118, -908.8039, -1364.591, -1733.045, -1962.697, -2010.432, -1855.975])\n",
    "\n",
    "x_filt_mac = signal.sosfilt(sos_ellip, x)\n",
    "\n",
    "x = x-np.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = x[150:]\n",
    "sig_filt = x_filt[150:]\n",
    "\n",
    "adc_to_v = lambda x: 3.6*(x/(2**12-1))\n",
    "\n",
    "t_ax = np.arange(len(sig))/256\n",
    "\n",
    "sig_ideal = (4/np.pi)*np.sin(2*np.pi*12*t_ax)\n",
    "\n",
    "fig, ax0 = plt.subplots(1, figsize=(14,5))\n",
    "ax0.plot(t_ax, adc_to_v(sig))\n",
    "ax0.plot(t_ax, adc_to_v(sig_filt))\n",
    "ax0.plot(t_ax, sig_ideal)\n",
    "\n",
    "ax0.spines['right'].set_visible(False)\n",
    "ax0.spines['top'].set_visible(False)\n",
    "\n",
    "ax0.set_xlabel('time (s)')\n",
    "ax0.set_ylabel('amplitude (V)')\n",
    "ax0.set_ylim(-2, 2.2)\n",
    "ax0.axhline(0, c='k', lw='0.5', ls='-.', alpha=0.3)\n",
    "\n",
    "ax0.legend(['$x\\,[n]$', '$y\\,[n]$', '$y^*[n]$'], ncol=3,loc='upper right', fontsize=14)\n",
    "plt.savefig('plots/sq_wave_filtering_time.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(18,5))\n",
    "# plot_periodogram(x_filt_ds, 64, N=256, figsize=(14, 8), axes=axes)\n",
    "plot_periodogram(x, 256, N=512, figsize=(10, 8), axes=axes, show_titles=False)\n",
    "plot_periodogram(x_filt, 256, N=512, axes=axes, show_titles=False)\n",
    "# plot_periodogram(x_ds_no_filt, 64, N=256, figsize=(14, 8), axes=axes)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.legend(['$\\hat{P}_x\\,(\\omega)$', '$\\hat{P}_y\\,(\\omega)$'], fontsize=19, ncol=2, loc='upper right')\n",
    "    ax.axvline(12, ls='-.', alpha=0.75, c='k')\n",
    "    ax.axvline(36, ls='-.', alpha=0.5, c='k')\n",
    "    ax.axvline(60, ls='-.', alpha=0.25, c='k')\n",
    "    ax.axvline(7*12, ls='-.', alpha=0.15, c='k')\n",
    "    ax.set_xticks(np.arange(0, 134, step=12))\n",
    "    ax.grid(1)\n",
    "\n",
    "plt.tight_layout(pad=1.8)\n",
    "plt.savefig('plots/sq_wave_filtering_spectra.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ds = x_filt[::4]\n",
    "x_ds_no_filt = x[::4]\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(18,5))\n",
    "\n",
    "plot_periodogram(x, 256, N=512, axes=axes, show_titles=False)\n",
    "plot_periodogram(x_ds, 64, N=512, figsize=(10, 8), axes=axes, show_titles=False)\n",
    "plot_periodogram(x_ds_no_filt, 64, N=512, axes=axes, show_titles=False)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.grid(b=1)\n",
    "    ax.legend(['$\\hat{P}_x\\,(\\omega)$', '$\\hat{P}_z\\,(\\omega)$', '$\\hat{P}_{alias}\\,(\\omega)$'], fontsize=18, ncol=3, loc='lower left')\n",
    "    ax.set_xlim(0, 34)\n",
    "    \n",
    "plt.savefig('plots/sq_wave_filtering_ds_spectra.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "fs = 256\n",
    "filt_ord = 10\n",
    "pb_rip = 0.2\n",
    "sb_atten = 80\n",
    "\n",
    "fc_lo = 4 # pass band lower freq\n",
    "fc_hi = 28 # pass band upp freq \n",
    "wc_lo = fc_lo/(fs*0.5)\n",
    "wc_hi = fc_hi/(fs*0.5)\n",
    "\n",
    "\n",
    "# sos_ellip = signal.ellip(filt_ord, pb_rip, sb_atten, (wc_lo, wc_hi), btype='bandpass', output='sos')\n",
    "sos_ellip = signal.ellip(filt_ord, pb_rip, sb_atten, wc_hi, btype='lowpass', output='sos')\n",
    "sos_cheby2 = signal.cheby2(filt_ord, sb_atten, wc_hi, btype='lowpass', output='sos')\n",
    "sos_cheby1 = signal.cheby1(filt_ord, pb_rip, wc_hi, btype='lowpass', output='sos')\n",
    "sos_bw = signal.butter(10, [1, 28], 'bp', fs=256, output='sos')\n",
    "\n",
    "w, h_ellip = signal.sosfreqz(sos_ellip, worN=512)\n",
    "_, h_cheby1 = signal.sosfreqz(sos_cheby1, worN=512)\n",
    "_, h_cheby2 = signal.sosfreqz(sos_cheby2, worN=512)\n",
    "\n",
    "# plot frequency response\n",
    "\n",
    "fig_mag = plt.figure(figsize=(14,8))\n",
    "plt.subplot(2, 1, 1)\n",
    "db = 20*np.log10(np.maximum(np.abs(h_ellip), 1e-5))\n",
    "plt.plot(w/np.pi, db, label='elliptical')\n",
    "\n",
    "db = 20*np.log10(np.maximum(np.abs(h_cheby1), 1e-5))\n",
    "plt.plot(w/np.pi, db, label='cheby. type I')\n",
    "\n",
    "db = 20*np.log10(np.maximum(np.abs(h_cheby2), 1e-5))\n",
    "plt.plot(w/np.pi, db, label='cheby. type II')\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "plt.ylim(-100, 5)\n",
    "plt.grid(True)\n",
    "plt.yticks([0, -8, -20, -40, -80, -100])\n",
    "plt.axvline(50/128, ls='-.', c='k', label='50Hz at $f_s=256$Hz', alpha=0.7)\n",
    "plt.ylabel('Gain (dB)')\n",
    "# plt.title('Frequency Response')\n",
    "plt.legend(ncol=4, fontsize=14)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(w/np.pi, np.angle(h_ellip), label='elliptical')\n",
    "plt.plot(w/np.pi, np.angle(h_cheby1), label='cheby. type I', alpha=0.4)\n",
    "plt.plot(w/np.pi, np.angle(h_cheby2), label='cheby. type II', alpha=0.4)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.axvline(50/128, ls='-.', c='k', label='50Hz at $f_s=256$Hz', alpha=0.7)\n",
    "\n",
    "plt.yticks([-np.pi, -0.5*np.pi, 0, 0.5*np.pi, np.pi],\n",
    "           [r'$-\\pi$', r'$-\\pi/2$', '0', r'$\\pi/2$', r'$\\pi$'])\n",
    "plt.ylabel('Phase (rad)')\n",
    "plt.xlabel('Normalized frequency (1.0 = Nyquist)')\n",
    "\n",
    "plt.legend(ncol=4, fontsize=14)    \n",
    "\n",
    "plt.tight_layout(pad=1.8)\n",
    "plt.savefig('plots/digital-filt-resp.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
